Presentation point of view:-
In an A/B test, we compare the performance of two or more versions of a site. When designing the experiment, several questions can emerge:-

How many different versions should be tested?
What kind of changes can we implement in each version of the test (from changing just the color of a button to redesigning the whole site)?
How can we show one version to a selected group of users and another version to a different group?
Which metric should we chose to compare the different versions?
Should Eniac experiment with other elements of the site instead (or in addition to) the “SHOP NOW” button?
How can we track, store and analyze the data from each version?
How can we ever be sure that the version with the best performance is not having more clicks due to just chance?
How long can we expect the experiment to last?
