Presentation point of view:-
In an A/B test, we compare the performance of two or more versions of a site. When designing the experiment, several questions can emerge:-

1. How many different versions should be tested?
2. What kind of changes can we implement in each version of the test (from changing just the color of a button to redesigning the whole site)?
3. How can we show one version to a selected group of users and another version to a different group?
4. Which metric should we chose to compare the different versions?
5. Should Eniac experiment with other elements of the site instead (or in addition to) the “SHOP NOW” button?
6. How can we track, store and analyze the data from each version?
7. How can we ever be sure that the version with the best performance is not having more clicks due to just chance?
8. How long can we expect the experiment to last?

A/B Testing: Improving UX Experience
From descriptive to inferential statistics 1. The Central Limit Theorem
2. The Standard Error
3. The t-Distribution - 1 sample t testing or 2 sample t testing
4. Hypothesis Testing
5. Chi-square Test

Project :- The Library of Montana State University
In an A/B Test, one of the tasks that usually belongs to the UX team is to perform user research and develop a new version of the website element that needs to be tested. The team had conversations with a few students and asked the following questions:

Have you previously clicked on Interact?
What content do you expect to see after you select Interact?
Does Interact accurately describe the content that you find after selecting Interact?
Which word best describes this category? Interact? Connect? Learn? Help? Services?

Would you include all suggested variants in the experiment (Connect, Learn, Help, Services)?
What is the “business value” that performing this experiment would add within the broader strategy of the University?
Which main metric would you choose to measure the success of a variant and perform the experiment on?
Which additional metrics would you choose to track?
How would you define the null and the alternative hypotheses?
What threshold for statistical significance would you set?
What is the minimum detectable effect (the smallest improvement you would care about) that you expect to detect?
Do you think this experiment would require a software engineering team to develop a custom platform, or could it be developed with external tools such as Google Optimize?

Explore the data and tackle these questions:

What was the click-through rate for each version?
Which version was the winner?
Do the results seem conclusive?
